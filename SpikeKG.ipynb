{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SpikeKG.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1LK7ITi_tLI4gYzpKG43vKvxMRjVB3u_g",
      "authorship_tag": "ABX9TyM0FVaoO+6M+iEqaiMqJB4i",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zhangxingeng/SpikeKG/blob/main/SpikeKG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Here is the source of the countries dataset [countries](https://github.com/ZhenfengLei/KGDatasets/tree/master/Countries)"
      ],
      "metadata": {
        "id": "1Xbs_cOCRg0-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "oXUM0kdmb9-9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1a7348a-000f-49e0-c912-e9600b10eb6b"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os, time\n",
        "import torch\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "OW-_YqZoKuPf"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Loading"
      ],
      "metadata": {
        "id": "LboLiNelYtrx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_rel2id(datapath, fname):\n",
        "  '''\n",
        "  Convenience function returning a dictionary that turns relation names into ids.\n",
        "  '''\n",
        "  file = open(os.path.join(datapath, fname))\n",
        "  content = file.readlines()\n",
        "  ids, nodenames = [], []\n",
        "  for i in range(len(content)):\n",
        "    a = content[i].split('\\t')\n",
        "    ids.append(int(a[0]))\n",
        "    nodenames.append(a[-1][:-1])\n",
        "  rel2id = dict(zip(nodenames, ids))\n",
        "  return rel2id\n",
        "\n",
        "def get_id2rel(datapath):\n",
        "  '''\n",
        "  Convenience function returning a dictionary that turns ids into relation names.\n",
        "  '''\n",
        "  rel2id = get_rel2id(datapath)\n",
        "  id2rel = dict(zip(rel2id.values(), rel2id.keys()))\n",
        "  return id2rel\n",
        "\n",
        "def get_ent2id(datapath, fname):\n",
        "  '''\n",
        "  Convenience function returning a dictionary that turns entity names into ids.\n",
        "  '''\n",
        "  file = open(os.path.join(datapath, fname))\n",
        "  content = file.readlines()\n",
        "  ids, nodenames = [], []\n",
        "  for i in range(len(content)):\n",
        "    a = content[i].split('\\t')\n",
        "    ids.append(int(a[0]))\n",
        "    nodenames.append(a[-1][:-1])\n",
        "  ent2id = dict(zip(nodenames, ids))\n",
        "  return ent2id\n",
        "\n",
        "def get_id2ent(datapath, fname):\n",
        "  '''\n",
        "  Convenience function returning a dictionary that turns ids into entity names.\n",
        "  '''\n",
        "  ent2id = get_ent2id(datapath, fname)\n",
        "  id2ent = dict(zip(ent2id.values(), ent2id.keys()))\n",
        "  return id2ent\n",
        "\n",
        "def load_data(datapath):\n",
        "  train_data = np.array(np.loadtxt(os.path.join(datapath, 'train.del')), dtype=int)\n",
        "  valid_data = np.array(np.loadtxt(os.path.join(datapath, 'valid.del')), dtype=int)\n",
        "  node_num = np.max([np.max(train_data[:,0]), np.max(train_data[:,2])])+1\n",
        "  predicate_num = np.max(train_data[:,1])+1\n",
        "  return train_data, valid_data, node_num, predicate_num"
      ],
      "metadata": {
        "id": "jXywykSdKyqR"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Filters For avoid direct result in training dataset"
      ],
      "metadata": {
        "id": "AuR-Rbj4Yjrk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_data_sp(train_data, valid_data, node_num):\n",
        "  '''\n",
        "  Filters for objects.\n",
        "  Given a triple spo, all objects x are removed that\n",
        "  appear as spx in the training or validation data.\n",
        "  '''\n",
        "  filtered_sp = [[] for i in range(len(valid_data))]\n",
        "  for i in range(len(valid_data)):\n",
        "    subj, pred, obj = valid_data[i]\n",
        "    for tr in train_data:\n",
        "      if tr[0] == subj and tr[1] == pred:\n",
        "        filtered_sp[i].append(tr[2])\n",
        "    for tr in valid_data:\n",
        "      if tr[0] == subj and tr[1] == pred:\n",
        "        filtered_sp[i].append(tr[2])\n",
        "    filtered_sp[i] = list(set(range(node_num)).difference(set(filtered_sp[i])))\n",
        "    filtered_sp[i] = [obj] + filtered_sp[i]\n",
        "  return filtered_sp\n",
        "\n",
        "def filter_data_po(train_data, valid_data, node_num):\n",
        "  '''\n",
        "  Filters for subjects.\n",
        "  Given a triple spo, all subjects x are removed that\n",
        "  appear as xpo in the training or validation data.\n",
        "  '''\n",
        "  filtered_po = [[] for i in range(len(valid_data))]\n",
        "  for i in range(len(valid_data)):\n",
        "    subj, pred, obj = valid_data[i]\n",
        "    for tr in train_data:\n",
        "      if tr[2] == obj and tr[1] == pred:\n",
        "        filtered_po[i].append(tr[0])\n",
        "    for tr in valid_data:\n",
        "      if tr[2] == obj and tr[1] == pred:\n",
        "        filtered_po[i].append(tr[0])\n",
        "    filtered_po[i] = list(set(range(node_num)).difference(set(filtered_po[i])))\n",
        "    filtered_po[i] = [subj] + filtered_po[i]\n",
        "  return filtered_po\n",
        "  \n",
        "def filter_data(datapath):\n",
        "  '''\n",
        "  Create filters for data (neglect known statements that are ranked\n",
        "  higher during testing than the evaluated triple).\n",
        "  '''\n",
        "  train_data, valid_data, node_num, _ = load_data(datapath)\n",
        "\n",
        "  valid_filter_sp = filter_data_sp(train_data, valid_data, node_num)\n",
        "  valid_filter_po = filter_data_po(train_data, valid_data, node_num)\n",
        "  train_filter_sp = filter_data_sp(valid_data, train_data, node_num)\n",
        "  train_filter_po = filter_data_po(valid_data, train_data, node_num)\n",
        "\n",
        "  np.save(os.path.join(datapath, 'valid_filter_sp'), valid_filter_sp)\n",
        "  np.save(os.path.join(datapath, 'valid_filter_po'), valid_filter_po)\n",
        "  np.save(os.path.join(datapath, 'train_filter_sp'), train_filter_sp)\n",
        "  np.save(os.path.join(datapath, 'train_filter_po'), train_filter_po)"
      ],
      "metadata": {
        "id": "u4rak7QNVw13"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Batch Creation"
      ],
      "metadata": {
        "id": "LOOGSLqBcPyy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from copy import deepcopy\n",
        "class batch_provider:\n",
        "  def __init__(self, data, batchsize, num_negSamples = 2, seed = 1231245):\n",
        "    '''\n",
        "    Helper class to provide data in batches with negative examples.\n",
        "    data: Training data triples\n",
        "    batchsize: size of the mini-batches\n",
        "    num_negSamples: number of neg. samples.\n",
        "    seed: random seed for neg. sample generation\n",
        "    '''\n",
        "    self.data = deepcopy(data)\n",
        "    self.node_num = np.max([np.max(data[:,0]), np.max(data[:,2])])\n",
        "\n",
        "    np.random.seed(seed)\n",
        "    np.random.shuffle(self.data)\n",
        "\n",
        "    self.batchsize = batchsize\n",
        "    self.number_minibatches = int(len(self.data)/batchsize)\n",
        "    self.current_minibatch = 0\n",
        "\n",
        "    self.num_negSamples = num_negSamples\n",
        "\n",
        "  def next_batch(self):\n",
        "    '''\n",
        "    Return the next mini-batch.\n",
        "    Data triples are shuffled after each epoch.\n",
        "    '''\n",
        "    i = self.current_minibatch\n",
        "    di = self.batchsize\n",
        "    mbatch = deepcopy(self.data[i*di:(i+1)*di])\n",
        "    self.current_minibatch += 1\n",
        "    if self.current_minibatch == self.number_minibatches:\n",
        "      np.random.shuffle(self.data)\n",
        "      self.current_minibatch = 0\n",
        "    if self.num_negSamples > 0:\n",
        "      subj, pred, obj, labels = self.apply_neg_examples(list(mbatch[:,0]), list(mbatch[:,1]), list(mbatch[:,2]))\n",
        "      return subj, pred, obj, labels\n",
        "    else:\n",
        "      return mbatch[:,0], mbatch[:,1], mbatch[:,2]\n",
        "\n",
        "  def apply_neg_examples(self, subj, pred, obj):\n",
        "    '''\n",
        "    Generate neg. samples for a mini-batch.\n",
        "    Both subject and object neg. samples are generated.\n",
        "    '''\n",
        "    vsize = len(subj)\n",
        "    labels = np.array([1 for i in range(vsize)] + [-1 for i in range(self.num_negSamples*2*vsize)])\n",
        "    neg_subj = list(np.random.randint(self.node_num, size = self.num_negSamples*vsize))\n",
        "    neg_obj = list(np.random.randint(self.node_num, size = self.num_negSamples*vsize))\n",
        "    return np.concatenate([subj, neg_subj, subj*self.num_negSamples]), np.concatenate([pred*(2*self.num_negSamples+1)]), np.concatenate([obj, obj*self.num_negSamples, neg_obj]), labels\n"
      ],
      "metadata": {
        "id": "O9s5T6GUa8lb"
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train and Eval"
      ],
      "metadata": {
        "id": "APDDMK98qeQ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy.stats as ss\n",
        "\n",
        "def score_triple(rel2id, ent2id, model, subj, pred, obj):\n",
        "    '''\n",
        "    Score single triple. Takes text name of graph entities as input.\n",
        "    '''\n",
        "    subj = torch.tensor([ent2id[subj]])\n",
        "    obj = torch.tensor([ent2id[obj]])\n",
        "    pred = torch.tensor([rel2id[pred]])\n",
        "    return model.score(subj, pred, obj)\n",
        "\n",
        "def get_rank_sp(model, data, whichOne, dataPath, savePath = None):\n",
        "    '''\n",
        "    Get metrics like mean rank, MRR, hits@k when the object is replaced.\n",
        "\n",
        "    model: trained model used to evaluate data\n",
        "    data: triples to be tested\n",
        "    whichOne: which set is used (train, valid)\n",
        "    dataPath: path of data folder\n",
        "    savePath: if not None, path where results are stored\n",
        "    '''\n",
        "    ranks = []\n",
        "    filtered_sp = np.load('{}/{}_filter_sp.npy'.format(dataPath, whichOne), allow_pickle=True)\n",
        "    for i in range(len(data)):\n",
        "        subj, pred = data[i][0], data[i][1]\n",
        "        rankings = model.score([subj for j in range(len(filtered_sp[i]))], [pred for j in range(len(filtered_sp[i]))], filtered_sp[i]).detach().numpy()\n",
        "        rank = ss.rankdata(-rankings)[0]\n",
        "        ranks.append(rank)\n",
        "    ranks_modes = [np.percentile(ranks, 25), np.median(ranks), np.percentile(ranks, 75), np.mean(ranks), np.mean(1/(np.array(ranks)))]\n",
        "    hitsAt = []\n",
        "    for hit in [1, 3, 10, 100, 500]: hitsAt.append(np.mean(np.array(ranks) <= hit))\n",
        "    if savePath is None: return ranks, ranks_modes, hitsAt\n",
        "    else:\n",
        "        save_ranks(ranks, ranks_modes, hitsAt, savePath, whichOne, 'sp')\n",
        "\n",
        "def get_rank_po(model, data, whichOne, dataPath, savePath = None):\n",
        "    '''\n",
        "    Get metrics like mean rank, MRR, hits@k when the subject is replaced.\n",
        "\n",
        "    model: trained model used to evaluate data\n",
        "    data: triples to be tested\n",
        "    whichOne: which set is used (train, valid)\n",
        "    dataPath: path of data folder\n",
        "    savePath: if not None, path where results are stored\n",
        "    '''\n",
        "    ranks = []\n",
        "    filtered_po = np.load('{}/{}_filter_po.npy'.format(dataPath, whichOne), allow_pickle=True)\n",
        "    for i in range(len(filtered_po)):\n",
        "        pred, obj = data[i][1], data[i][2]\n",
        "        rankings = model.score(filtered_po[i], [pred for j in range(len(filtered_po[i]))], [obj for j in range(len(filtered_po[i]))]).detach().numpy()\n",
        "        rank= ss.rankdata(-rankings)[0]\n",
        "        ranks.append(rank)\n",
        "    ranks_modes = [np.percentile(ranks, 25), np.median(ranks), np.percentile(ranks, 75), np.mean(ranks), np.mean(1/(np.array(ranks)))]\n",
        "    hitsAt = []\n",
        "    for hit in [1, 3, 10, 100, 500]: hitsAt.append(np.mean(np.array(ranks) <= hit))\n",
        "    if savePath is None: return ranks, ranks_modes, hitsAt\n",
        "    else: \n",
        "        save_ranks(ranks, ranks_modes, hitsAt, savePath, whichOne, 'po')\n",
        "\n",
        "def save_ranks(ranks, ranks_modes, hitsAt, savePath, whichOne, mode):\n",
        "    '''\n",
        "    Save rank metric results.\n",
        "    '''\n",
        "    plt.close()\n",
        "    plt.bar(np.arange(len(hitsAt)+1), hitsAt+[ranks_modes[-1]])\n",
        "    plt.xticks(np.arange(len(hitsAt)+1), [1, 3, 10, 100, 500, 'MRR'])\n",
        "    plt.ylim(0,1)\n",
        "    plt.savefig('{}/{}_{}_hits_and_RMR.png'.format(savePath, whichOne, mode))\n",
        "    np.savetxt('{}/{}_{}_hits.txt'.format(savePath, whichOne, mode), [[1, 3, 10, 100, 500], hitsAt])\n",
        "    np.savetxt('{}/{}_{}_ranking.txt'.format(savePath, whichOne, mode), ranks)\n",
        "    np.savetxt('{}/{}_{}_ranking_modes.txt'.format(savePath, whichOne, mode), ranks_modes)\n",
        "\n",
        "def get_scores(model, train_data, valid_data, neg_data, savePath, post=''):\n",
        "    '''\n",
        "    Calculate and save scores of training data, validation data and a set of negative triples.\n",
        "    '''\n",
        "    train_scores = model.score(train_data[:,0], train_data[:,1], train_data[:,2]).detach().numpy()\n",
        "    valid_scores = model.score(valid_data[:,0], valid_data[:,1], valid_data[:,2]).detach().numpy()\n",
        "    neg_scores = model.score(neg_data[:,0], neg_data[:,1], neg_data[:,2]).detach().numpy()\n",
        "\n",
        "    plt.close()\n",
        "    _ = plt.hist(train_scores, bins = 100, alpha = 0.35, color = 'gray', density=True, label = 'train')\n",
        "    _ = plt.hist(neg_scores, bins = 100, density=True, alpha = 0.7, histtype='step', linewidth = 2.5, label = 'neg')\n",
        "    _ = plt.hist(valid_scores, bins = 100, density=True, alpha = 0.7, histtype='step', linewidth = 2.5, label = 'pos')\n",
        "    plt.legend()\n",
        "    plt.xlim(-10,.1)\n",
        "    plt.savefig('{}/{}score_histogram.png'.format(savePath, post))\n",
        "    np.save('{}/{}train_scores.npy'.format(savePath, post), train_scores)\n",
        "    np.save('{}/{}valid_scores.npy'.format(savePath, post), valid_scores)\n",
        "    np.save('{}/{}neg_scores.npy'.format(savePath, post), neg_scores)\n"
      ],
      "metadata": {
        "id": "cNkHzory7zZY"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(optimizer, batcher, model, delta, steps, data = None):\n",
        "  if data is not None:\n",
        "    train_data, valid_data, neg_data = data[0], data[1], data[2]\n",
        "  loss_fun = torch.nn.SoftMarginLoss()\n",
        "\n",
        "  start_time = time.time()\n",
        "  for k in range(steps):\n",
        "    # Print scores\n",
        "    if k % 100 == 0:\n",
        "      estimate_time = (time.time()-start_time)/(k+1)*(steps-k)/60.0\n",
        "      if data is not None:\n",
        "        train_score = float(torch.mean(model.score(*train_data)).detach().numpy())\n",
        "        valid_score = float(torch.mean(model.score(*valid_data)).detach().numpy())\n",
        "        neg_score = float(torch.mean(model.score(*neg_data)).detach().numpy())\n",
        "        print('SpikE {}: ETA {}min; train: {}, valid: {}, neg: {}'.format(k, np.round(estimate_time,2), train_score, valid_score, neg_score))\n",
        "      else:\n",
        "        print('SpikE {}: ETA {}min'.format(k, np.round(estimate_time,2)))\n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    databatch = batcher.next_betch()\n",
        "    prediction = model.score(*databatch)\n",
        "    weight_reg = model.entities.weight_loss()\n",
        "    loss = loss_fun(prediction, torch.tensor(databatch[-1]))\n",
        "    loss = loss + delta*weight_reg\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    model.update_embeddings()\n",
        "\n",
        "\n",
        "def train_and_evaluate(optimizer, batcher, model, delta, steps, eval_points, datapath, data):\n",
        "    train_data, valid_data = data[0], data[1]\n",
        "    loss_fun = torch.nn.SoftMarginLoss()\n",
        "    starttime = time.time()\n",
        "    for k in range(steps):\n",
        "        # Evaluate current Model\n",
        "        if k in eval_points:\n",
        "            print('{}:\\n'.format(k))\n",
        "            _, ranks_modes, hitsAt = get_rank_sp(model, train_data, 'train', datapath)\n",
        "            print('train ~~~~SP~~~~ hits@1: {0:.4f} __hits@3: {1:.4f} __ mean: {2:.4f} __ MRR: {3:.4f}\\n'.format(hitsAt[0], hitsAt[1], ranks_modes[3], ranks_modes[4]))\n",
        "            _, ranks_modes, hitsAt = get_rank_po(model, train_data, 'train', datapath)\n",
        "            print('train ~~~~PO~~~~ hits@1: {0:.4f} __hits@3: {1:.4f} __ mean: {2:.4f} __ MRR: {3:.4f}\\n'.format(hitsAt[0], hitsAt[1], ranks_modes[3], ranks_modes[4]))\n",
        "            _, ranks_modes, hitsAt = get_rank_sp(model, valid_data, 'valid', datapath)\n",
        "            print('valid ~~~~SP~~~~ hits@1: {0:.4f} __hits@3: {1:.4f} __ mean: {2:.4f} __ MRR: {3:.4f}\\n'.format(hitsAt[0], hitsAt[1], ranks_modes[3], ranks_modes[4]))\n",
        "            _, ranks_modes, hitsAt = get_rank_po(model, valid_data, 'valid', datapath)\n",
        "            print('valid ~~~~PO~~~~ hits@1: {0:.4f} __hits@3: {1:.4f} __ mean: {2:.4f} __ MRR: {3:.4f}\\n'.format(hitsAt[0], hitsAt[1], ranks_modes[3], ranks_modes[4]))\n",
        "\n",
        "            eta = (time.time()-starttime)/(k+1)*(steps-k)/60.\n",
        "            print('ETA {0:.2f}min \\n\\n'.format(eta))\n",
        "        \n",
        "        # Update current model on new batch\n",
        "        optimizer.zero_grad()\n",
        "        databatch = batcher.next_batch()\n",
        "        prediction = model.score(databatch[0], databatch[1], databatch[2])\n",
        "        weight_reg = model.entities.weight_loss()\n",
        "        loss = loss_fun(prediction, torch.tensor(databatch[-1]))\n",
        "        loss = loss + delta*weight_reg\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        model.update_embeddings()\n"
      ],
      "metadata": {
        "id": "SfrM_gEfqVSp"
      },
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Neuron Model"
      ],
      "metadata": {
        "id": "twcpsFrZF9x4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from copy import deepcopy\n",
        "\n",
        "class nLIF:\n",
        "    def __init__(self, node_dim, embed_dim, input_dim, tau, maxSpan, seed):\n",
        "        torch.manual_seed(seed)\n",
        "        # neuron params\n",
        "        self.tau = tau\n",
        "        self.threshold = 1.\n",
        "        self.winit = 0.2\n",
        "        self.maxSpan = maxSpan/2\n",
        "\n",
        "        # network params\n",
        "        self.node_dim = node_dim\n",
        "        self.embed_dim = embed_dim\n",
        "        self.input_dim = input_dim\n",
        "\n",
        "        # Generate random input spikes with max height of maxSpan/2\n",
        "        self.input_spikes = (torch.rand(input_dim)-0.5) * maxSpan\n",
        "        self.input_spikes = self.input_spikes.sort().values\n",
        "        self.input_exp = torch.exp(self.input_spikes/self.tau)\n",
        "        # create spike sequence\n",
        "        self.spike_seq = torch.zeros((self.input_dim, self.input_dim))\n",
        "        for i in range(self.input_dim):\n",
        "            for j in range(self.input_dim): # create exp input spike at time step i\n",
        "                if i >= j:\n",
        "                    self.spike_seq[i][j] = self.input_exp[j]\n",
        "        self.spike_mask = (self.spike_seq > 0)*1. # binary value\n",
        "        \n",
        "        # create input -> population weights\n",
        "        self.weights = self.init_weights()\n",
        "        self.spike_times = self.get_spike_times()\n",
        "\n",
        "    def init_weights(self):\n",
        "        # Initialize weights using normal distribution.\n",
        "        weights = torch.nn.Embedding(self.node_dim, self.embed_dim*self.input_dim)\n",
        "        for i in range(self.node_dim):\n",
        "            # guarantee that all populations spike once\n",
        "            while bool((weights.weight.view(-1,self.embed_dim,self.input_dim)[i].sum(-1) < 1).any()) == True:\n",
        "                weights.weight.data[i] = torch.normal(mean = self.winit, std = 1, size = (1, self.embed_dim*self.input_dim))\n",
        "        return weights\n",
        "\n",
        "    def get_spike_times(self):\n",
        "        spike_times = torch.zeros((self.node_dim, self.embed_dim))+self.maxSpan+0.5\n",
        "        weights = self.weights.weight.view(-1, self.embed_dim, self.input_dim)\n",
        "        for j in range(self.input_dim):\n",
        "            wSumExp = torch.matmul(weights, self.spike_seq[j])\n",
        "            wSum = torch.matmul(weights, self.spike_mask[j])\n",
        "            wSumDiff = wSum - self.threshold\n",
        "            wQuotient = wSumExp/(wSumDiff+ 1e-10) + 1e-10 # for stability\n",
        "            times = self.tau * torch.log(wQuotient)\n",
        "            if j < self.input_dim-1:\n",
        "                # check condition for spiking\n",
        "                # 1. has not spiked yet\n",
        "                # 2. weights sum over threshold\n",
        "                # 3. next input spike does not hinder firing\n",
        "                new_spikes = (spike_times == self.maxSpan+0.5)*(wSum > self.threshold)*(wQuotient < self.input_exp[j+1])\n",
        "            else: # for last spike\n",
        "                new_spikes =  (spike_times == self.maxSpan+0.5)*(wSum > self.threshold)\n",
        "            spike_times[new_spikes] = times[new_spikes]\n",
        "        return spike_times\n",
        "\n",
        "    def update_embeddings(self):\n",
        "        ''' \n",
        "        Spike embeddings are stored after calculation to reduce compute time \n",
        "        when evaluating several times without training updates.\n",
        "        '''\n",
        "        self.spike_times = self.get_spike_times()\n",
        "\n",
        "    def embeddings(self, s_embs, o_embs):\n",
        "        '''\n",
        "        Read out embeddings.\n",
        "        Input: list of subjects, list of objects\n",
        "        Output: list of subject embeddings, list of object embeddings\n",
        "        '''\n",
        "        s_embs = torch.tensor(s_embs).long()\n",
        "        o_embs = torch.tensor(o_embs).long()\n",
        "        return self.spike_times[s_embs], self.spike_times[o_embs]\n",
        "\n",
        "    def weight_loss(self):\n",
        "        '''\n",
        "        Regularization term that increases weights when their sum is below the \n",
        "        threshold value.\n",
        "        Output: regularization term\n",
        "        '''\n",
        "        weight_norm = self.weights.weight.view(-1, self.embed_dim, self.input_dim).sum(-1)\n",
        "        return ((self.threshold-weight_norm)*(weight_norm < self.threshold)).sum()\n",
        "\n",
        "    def _integrate_model_using_Euler(self):\n",
        "        '''\n",
        "        Euler method to obtain spike times. Used to cross-check the analytical solution.\n",
        "        Output: spike times, membrane potentials over time\n",
        "        '''\n",
        "        results = []\n",
        "        spike_times = torch.zeros((self.node_dim, self.embed_dim))+1.5\n",
        "        voltage = torch.zeros((self.node_dim, self.embed_dim))\n",
        "        weights = self.weights.weight.view(-1, self.embed_dim, self.input_dim)\n",
        "\n",
        "        # Euler integration from t = -maxSpan to maxSpan to solve ODE\n",
        "        # return spike times + voltage traces\n",
        "        t = -self.maxSpan\n",
        "        dt = 0.01\n",
        "\n",
        "        results.append(deepcopy(voltage.detach().numpy()))\n",
        "        while t <= self.maxSpan:\n",
        "            voltage.data = torch.matmul(weights, (1-torch.exp(-(t-self.input_spikes)/self.tau)) * (t > self.input_spikes))\n",
        "            t += dt\n",
        "            results.append(deepcopy(voltage.detach().numpy()))\n",
        "\n",
        "            new_spikes = (t-dt-1.5)*(voltage > self.threshold)+1.5\n",
        "            spiked = spike_times == 1.5\n",
        "            spike_times = torch.logical_not(spiked)*spike_times + spiked*new_spikes\n",
        "\n",
        "        return spike_times, results\n",
        "\n",
        "\n",
        "torch.set_printoptions(precision=2)\n",
        "nlif = nLIF(4, 4, 2, 0.2, 10, 0)\n",
        "print(nlif.spike_times)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nmtu8X2xF9Ci",
        "outputId": "b4847c58-4fc6-4326-9df9-19aee132ebc0"
      },
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[2.75, 0.04, 0.14, 0.10],\n",
            "        [0.40, 0.12, 3.09, 0.30],\n",
            "        [2.98, 0.31, 3.00, 0.17],\n",
            "        [0.11, 2.98, 0.14, 0.19]], grad_fn=<IndexPutBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Network Model"
      ],
      "metadata": {
        "id": "Xl0jPAe93GN0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Scoring Helper Functions\n",
        "from torch.nn import functional as F\n",
        "\n",
        "def ASYmmetric_score(s_emb, o_emb, p_emb):\n",
        "  ''' TransE score '''\n",
        "  return -F.pairwise_distance(s_emb - o_emb, p_emb, p=1)\n",
        "\n",
        "def SYMmetric_score(s_emb, o_emb, p_emb):\n",
        "  ''' Symmetric version of TransE score '''\n",
        "  return -F.pairwise_distance((s_emb - o_emb).abs(), p_emb, p=1)"
      ],
      "metadata": {
        "id": "l1uTGIYFpoLF"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SpikE_Scorer_S:\n",
        "    def __init__(self, node_dim, embed_dim, input_dim, relation_dim, tau, maxSpan = 2, seed = 1231245):\n",
        "        torch.manual_seed(seed)\n",
        "        self.entities = nLIF(node_dim, embed_dim, input_dim, tau, maxSpan, seed)\n",
        "        self.predicates = torch.nn.Embedding(relation_dim, embed_dim)\n",
        "        self.node_num = node_dim\n",
        "    \n",
        "    def score(self, subj, pred, obj):\n",
        "        '''\n",
        "        Calculate the score of a list of triples.\n",
        "        Input: list of subjects, predicates, objects, e.g., [s0, s1, ...], [p0, p1, ...], [o0, o1, ...]\n",
        "        Output: list of scores\n",
        "        '''\n",
        "        s_emb, o_emb = self.entities.embeddings(subj, obj)\n",
        "        p_emb = self.predicates(torch.tensor(pred).long())\n",
        "        return SYMmetric_score(s_emb, o_emb, p_emb)\n",
        "\n",
        "    def update_embeddings(self):\n",
        "        self.entities.update_embeddings() # pass to nLIF\n",
        "    \n",
        "    def save(self, savepath, appdix = ''):\n",
        "        ''' Save some stuff. '''\n",
        "        pred_embs = self.predicates.weight.data.detach().numpy()\n",
        "        ent_embs = self.entities.get_spike_times().detach().numpy()\n",
        "        np.save('{}/weights_{}.npy'.format(savepath, appdix), self.entities.weights.weight.data.detach().numpy())\n",
        "        np.save('{}/input_spikes_{}.npy'.format(savepath, appdix), self.entities.input_spikes.detach().numpy())\n",
        "        np.save('{}/predicate_embeddings_{}.npy'.format(savepath, appdix), pred_embs)\n",
        "        np.save('{}/entity_embeddings_{}.npy'.format(savepath, appdix), ent_embs)\n",
        "\n",
        "        plt.close()\n",
        "        for j in range(50): plt.vlines(ent_embs[j], j+0.1, (j+1)-0.1)\n",
        "        plt.savefig('{}/entity_embeddings_{}.png'.format(savepath, appdix))\n",
        "\n",
        "        plt.close()\n",
        "        for j in range(len(pred_embs)): plt.vlines(pred_embs[j], j+0.1, (j+1)-0.1)\n",
        "        plt.savefig('{}/predicate_embeddings_{}.png'.format(savepath, appdix))\n",
        "\n",
        "class SpikE_Scorer_AS(SpikE_Scorer_S):\n",
        "    def __init__(self, node_dim, embed_dim, input_dim, relation_dim, tau, maxSpan = 2, seed = 1231245):\n",
        "        super().__init__(node_dim, embed_dim, input_dim, relation_dim, tau, maxSpan, seed)\n",
        "    \n",
        "    def score(self, subj, pred, obj):\n",
        "        '''\n",
        "        Calculate the score of a list of triples.\n",
        "        Input: list of subjects, predicates, objects, e.g., [s0, s1, ...], [p0, p1, ...], [o0, o1, ...]\n",
        "        Output: list of scores\n",
        "        '''\n",
        "        s_emb, o_emb = self.entities.embeddings(subj, obj)\n",
        "        p_emb = self.predicates(torch.tensor(pred).long())\n",
        "\n",
        "        return ASYmmetric_score(s_emb, o_emb, p_emb)\n",
        "\n"
      ],
      "metadata": {
        "id": "oAGB7Ufd3Fm9"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## New Stuff"
      ],
      "metadata": {
        "id": "XR3EvtTs7Xrx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from argparse import Namespace"
      ],
      "metadata": {
        "id": "vtSpHQya9xUF"
      },
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datapath = '/content/gdrive/MyDrive/SpikeKG/data/Countries_S1'\n",
        "ent2id = get_ent2id(datapath, 'entity_ids.del')\n",
        "rel2id = get_rel2id(datapath, 'relation_ids.del')\n",
        "id2ent = get_id2ent(datapath, 'entity_ids.del')\n",
        "train_data, valid_data, node_num, predicate_num = load_data(datapath)\n",
        "\n",
        "# print(f\"ent2id:{dict(list(ent2id.items())[:4])}...\")\n",
        "# print(f\"rel2id:{dict(list(rel2id.items())[:4])}...\")\n",
        "# print(f\"id2ent:{dict(list(id2ent.items())[:4])}...\")\n",
        "\n",
        "# print(f\"train-data shape:{np.shape(train_data)}, valid-data shape: {np.shape(valid_data)}\")\n",
        "# print(f\"total # ents: {node_num}, total # rels: {predicate_num}\")\n",
        "\n",
        "if not os.path.exists('{}/valid_filter_po.npy'.format(datapath)):\n",
        "    print('Creating data for filtered metrics...')\n",
        "    filter_data(datapath)\n",
        "\n",
        "params_dict = {\n",
        "  'embed_dim': 40,\n",
        "  'input_dim': 40,\n",
        "  'tau': 0.5,\n",
        "  'batchsize': 64,\n",
        "  'delta': 0.001,\n",
        "  'lr': .1,\n",
        "  'L2': 0.,\n",
        "  'steps': 801,\n",
        "  'neg_samples': 10,\n",
        "  'maxSpan': 2,\n",
        "}\n",
        "params = Namespace(**params_dict)\n",
        "\n",
        "eval_points = list(range(0, 1001, 200))\n",
        "seed = np.random.randint(1e8)\n",
        "\n",
        "batcher = batch_provider(train_data, params.batchsize, params.neg_samples, seed)\n",
        "\n",
        "model = SpikE_Scorer_AS(node_num, predicate_num, params.embed_dim, params.input_dim, \n",
        "                        params.tau, params.maxSpan, seed)\n",
        "optimizer = torch.optim.Adagrad([model.entities.weights.weight, model.predicates.weight], \n",
        "                                lr=params.lr, weight_decay = params.L2)\n",
        "train_and_evaluate(optimizer, batcher, model, params.delta, params.steps, eval_points, \n",
        "      datapath = datapath, data = [train_data, valid_data])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VEKMKxQN7dFW",
        "outputId": "e2e238db-e1c8-4d3c-a7eb-d2dec219083f"
      },
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating data for filtered metrics...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/lib/npyio.py:528: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  arr = np.asanyarray(arr)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0:\n",
            "\n",
            "train ~~~~SP~~~~ hits@1: 0.0009 __hits@3: 0.0054 __ mean: 139.2286 __ MRR: 0.0180\n",
            "\n",
            "train ~~~~PO~~~~ hits@1: 0.0036 __hits@3: 0.0072 __ mean: 131.1206 __ MRR: 0.0199\n",
            "\n",
            "valid ~~~~SP~~~~ hits@1: 0.0000 __hits@3: 0.0000 __ mean: 175.0833 __ MRR: 0.0112\n",
            "\n",
            "valid ~~~~PO~~~~ hits@1: 0.0000 __hits@3: 0.0000 __ mean: 103.6667 __ MRR: 0.0179\n",
            "\n",
            "ETA 14.17min \n",
            "\n",
            "\n",
            "200:\n",
            "\n",
            "train ~~~~SP~~~~ hits@1: 0.0648 __hits@3: 0.1323 __ mean: 35.3384 __ MRR: 0.1558\n",
            "\n",
            "train ~~~~PO~~~~ hits@1: 0.0198 __hits@3: 0.0576 __ mean: 60.0657 __ MRR: 0.0780\n",
            "\n",
            "valid ~~~~SP~~~~ hits@1: 0.1667 __hits@3: 0.3750 __ mean: 7.0417 __ MRR: 0.3475\n",
            "\n",
            "valid ~~~~PO~~~~ hits@1: 0.0000 __hits@3: 0.0000 __ mean: 98.2083 __ MRR: 0.0243\n",
            "\n",
            "ETA 0.26min \n",
            "\n",
            "\n",
            "400:\n",
            "\n",
            "train ~~~~SP~~~~ hits@1: 0.0819 __hits@3: 0.1872 __ mean: 28.2574 __ MRR: 0.1804\n",
            "\n",
            "train ~~~~PO~~~~ hits@1: 0.0171 __hits@3: 0.0693 __ mean: 53.0963 __ MRR: 0.0880\n",
            "\n",
            "valid ~~~~SP~~~~ hits@1: 0.1667 __hits@3: 0.3750 __ mean: 6.8750 __ MRR: 0.3420\n",
            "\n",
            "valid ~~~~PO~~~~ hits@1: 0.0000 __hits@3: 0.0000 __ mean: 86.6250 __ MRR: 0.0294\n",
            "\n",
            "ETA 0.15min \n",
            "\n",
            "\n",
            "600:\n",
            "\n",
            "train ~~~~SP~~~~ hits@1: 0.1071 __hits@3: 0.2367 __ mean: 28.4059 __ MRR: 0.2067\n",
            "\n",
            "train ~~~~PO~~~~ hits@1: 0.0162 __hits@3: 0.0801 __ mean: 46.4311 __ MRR: 0.0927\n",
            "\n",
            "valid ~~~~SP~~~~ hits@1: 0.0833 __hits@3: 0.6250 __ mean: 8.4167 __ MRR: 0.3541\n",
            "\n",
            "valid ~~~~PO~~~~ hits@1: 0.0000 __hits@3: 0.0833 __ mean: 58.9167 __ MRR: 0.0772\n",
            "\n",
            "ETA 0.07min \n",
            "\n",
            "\n",
            "800:\n",
            "\n",
            "train ~~~~SP~~~~ hits@1: 0.1170 __hits@3: 0.1899 __ mean: 27.3798 __ MRR: 0.1999\n",
            "\n",
            "train ~~~~PO~~~~ hits@1: 0.0270 __hits@3: 0.0693 __ mean: 47.8731 __ MRR: 0.0930\n",
            "\n",
            "valid ~~~~SP~~~~ hits@1: 0.4167 __hits@3: 0.6250 __ mean: 4.3333 __ MRR: 0.5678\n",
            "\n",
            "valid ~~~~PO~~~~ hits@1: 0.0000 __hits@3: 0.0417 __ mean: 62.6250 __ MRR: 0.0599\n",
            "\n",
            "ETA 0.00min \n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ]
}