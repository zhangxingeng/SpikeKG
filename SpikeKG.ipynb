{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SpikeKG.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1LK7ITi_tLI4gYzpKG43vKvxMRjVB3u_g",
      "authorship_tag": "ABX9TyPoGJ87cHkNa/iAyANABNOx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zhangxingeng/SpikeKG/blob/main/SpikeKG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Here is the source of the countries dataset [countries](https://github.com/ZhenfengLei/KGDatasets/tree/master/Countries)"
      ],
      "metadata": {
        "id": "1Xbs_cOCRg0-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "oXUM0kdmb9-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os"
      ],
      "metadata": {
        "id": "OW-_YqZoKuPf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Loading"
      ],
      "metadata": {
        "id": "LboLiNelYtrx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_rel2id(datapath, fname):\n",
        "  '''\n",
        "  Convenience function returning a dictionary that turns relation names into ids.\n",
        "  '''\n",
        "  file = open(os.path.join(datapath, fname))\n",
        "  content = file.readlines()\n",
        "  ids, nodenames = [], []\n",
        "  for i in range(len(content)):\n",
        "    a = content[i].split('\\t')\n",
        "    ids.append(int(a[0]))\n",
        "    nodenames.append(a[-1][:-1])\n",
        "  rel2id = dict(zip(nodenames, ids))\n",
        "  return rel2id\n",
        "\n",
        "def get_id2rel(datapath):\n",
        "  '''\n",
        "  Convenience function returning a dictionary that turns ids into relation names.\n",
        "  '''\n",
        "  rel2id = get_rel2id(datapath)\n",
        "  id2rel = dict(zip(rel2id.values(), rel2id.keys()))\n",
        "  return id2rel\n",
        "\n",
        "def get_ent2id(datapath, fname):\n",
        "  '''\n",
        "  Convenience function returning a dictionary that turns entity names into ids.\n",
        "  '''\n",
        "  file = open(os.path.join(datapath, fname))\n",
        "  content = file.readlines()\n",
        "  ids, nodenames = [], []\n",
        "  for i in range(len(content)):\n",
        "    a = content[i].split('\\t')\n",
        "    ids.append(int(a[0]))\n",
        "    nodenames.append(a[-1][:-1])\n",
        "  ent2id = dict(zip(nodenames, ids))\n",
        "  return ent2id\n",
        "\n",
        "def get_id2ent(datapath, fname):\n",
        "  '''\n",
        "  Convenience function returning a dictionary that turns ids into entity names.\n",
        "  '''\n",
        "  ent2id = get_ent2id(datapath, fname)\n",
        "  id2ent = dict(zip(ent2id.values(), ent2id.keys()))\n",
        "  return id2ent\n",
        "\n",
        "def load_data(datapath):\n",
        "  train_data = np.array(np.loadtxt(os.path.join(datapath, 'train.del')), dtype=int)\n",
        "  valid_data = np.array(np.loadtxt(os.path.join(datapath, 'valid.del')), dtype=int)\n",
        "  num_nodes = np.max([np.max(train_data[:,0]), np.max(train_data[:,2])])+1\n",
        "  num_predicates = np.max(train_data[:,1])+1\n",
        "  return train_data, valid_data, num_nodes, num_predicates"
      ],
      "metadata": {
        "id": "jXywykSdKyqR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datapath = '/content/gdrive/MyDrive/SpikeKG/data/Countries_S1'\n",
        "ent2id = get_ent2id(datapath, 'entity_ids.del')\n",
        "rel2id = get_rel2id(datapath, 'relation_ids.del')\n",
        "id2ent = get_id2ent(datapath, 'entity_ids.del')\n",
        "train_data, valid_data, num_nodes, num_predicates = load_data(datapath)\n",
        "\n",
        "print(f\"ent2id:{dict(list(ent2id.items())[:4])}...\")\n",
        "print(f\"rel2id:{dict(list(rel2id.items())[:4])}...\")\n",
        "print(f\"id2ent:{dict(list(id2ent.items())[:4])}...\")\n",
        "\n",
        "print(f\"train-data shape:{np.shape(train_data)}, valid-data shape: {np.shape(valid_data)}\")\n",
        "print(f\"total # ents: {num_nodes}, total # rels: {num_predicates}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MXYB7yIFb7ge",
        "outputId": "43c38b62-e68c-489d-cf7d-93399ee07496"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ent2id:{'western_africa': 0, 'africa': 1, 'slovakia': 2, 'ukraine': 3}...\n",
            "rel2id:{'locatedin': 0, 'neighbor': 1}...\n",
            "id2ent:{0: 'western_africa', 1: 'africa', 2: 'slovakia', 3: 'ukraine'}...\n",
            "train-data shape:(1111, 3), valid-data shape: (24, 3)\n",
            "total # ents: 271, total # rels: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Filters For avoid direct result in training dataset"
      ],
      "metadata": {
        "id": "AuR-Rbj4Yjrk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_data_sp(train_data, valid_data, num_nodes):\n",
        "  '''\n",
        "  Filters for objects.\n",
        "  Given a triple spo, all objects x are removed that\n",
        "  appear as spx in the training or validation data.\n",
        "  '''\n",
        "  filtered_sp = [[] for i in range(len(valid_data))]\n",
        "  for i in range(len(valid_data)):\n",
        "    subj, pred, obj = valid_data[i]\n",
        "    for tr in train_data:\n",
        "      if tr[0] == subj and tr[1] == pred:\n",
        "        filtered_sp[i].append(tr[2])\n",
        "    for tr in valid_data:\n",
        "      if tr[0] == subj and tr[1] == pred:\n",
        "        filtered_sp[i].append(tr[2])\n",
        "    filtered_sp[i] = list(set(range(num_nodes)).difference(set(filtered_sp[i])))\n",
        "    filtered_sp[i] = [obj] + filtered_sp[i]\n",
        "  return np.array(filtered_sp, dtype=object)\n",
        "\n",
        "def filter_data_po(train_data, valid_data, num_nodes):\n",
        "  '''\n",
        "  Filters for subjects.\n",
        "  Given a triple spo, all subjects x are removed that\n",
        "  appear as xpo in the training or validation data.\n",
        "  '''\n",
        "  filtered_po = [[] for i in range(len(valid_data))]\n",
        "  for i in range(len(valid_data)):\n",
        "    subj, pred, obj = valid_data[i]\n",
        "    for tr in train_data:\n",
        "      if tr[2] == obj and tr[1] == pred:\n",
        "        filtered_po[i].append(tr[0])\n",
        "    for tr in valid_data:\n",
        "      if tr[2] == obj and tr[1] == pred:\n",
        "        filtered_po[i].append(tr[0])\n",
        "    filtered_po[i] = list(set(range(num_nodes)).difference(set(filtered_po[i])))\n",
        "    filtered_po[i] = [subj] + filtered_po[i]\n",
        "  return np.array(filtered_po, dtype=object)\n",
        "  \n",
        "def filter_data(datapath):\n",
        "  '''\n",
        "  Create filters for data (neglect known statements that are ranked\n",
        "  higher during testing than the evaluated triple).\n",
        "  '''\n",
        "  train_data, valid_data, num_nodes, _ = load_data(datapath)\n",
        "\n",
        "  valid_filter_sp = filter_data_sp(train_data, valid_data, num_nodes)\n",
        "  valid_filter_po = filter_data_po(train_data, valid_data, num_nodes)\n",
        "  train_filter_sp = filter_data_sp(valid_data, train_data, num_nodes)\n",
        "  train_filter_po = filter_data_po(valid_data, train_data, num_nodes)\n",
        "\n",
        "  np.save(os.path.join(datapath, 'valid_filter_sp'), valid_filter_sp)\n",
        "  np.save(os.path.join(datapath, 'valid_filter_po'), valid_filter_po)\n",
        "  np.save(os.path.join(datapath, 'train_filter_sp'), train_filter_sp)\n",
        "  np.save(os.path.join(datapath, 'train_filter_po'), train_filter_po)"
      ],
      "metadata": {
        "id": "u4rak7QNVw13"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.exists('{}/valid_filter_po.npy'.format(datapath)):\n",
        "  filter_data(datapath)\n",
        "  print('Filtered metrics created.')"
      ],
      "metadata": {
        "id": "xKk6DuVaYx39"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Batch Creation"
      ],
      "metadata": {
        "id": "LOOGSLqBcPyy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from copy import deepcopy\n",
        "class batch_provider:\n",
        "  def __init__(self, data, batchsize, num_negSamples = 2, seed = 1231245):\n",
        "    '''\n",
        "    Helper class to provide data in batches with negative examples.\n",
        "    data: Training data triples\n",
        "    batchsize: size of the mini-batches\n",
        "    num_negSamples: number of neg. samples.\n",
        "    seed: random seed for neg. sample generation\n",
        "    '''\n",
        "    self.data = deepcopy(data)\n",
        "    self.num_nodes = np.max([np.max(data[:,0]), np.max(data[:,2])])\n",
        "\n",
        "    np.random.seed(seed)\n",
        "    np.random.shuffle(self.data)\n",
        "\n",
        "    self.batchsize = batchsize\n",
        "    self.number_minibatches = int(len(self.data)/batchsize)\n",
        "    self.current_minibatch = 0\n",
        "\n",
        "    self.num_negSamples = num_negSamples\n",
        "\n",
        "  def next_batch(self):\n",
        "    '''\n",
        "    Return the next mini-batch.\n",
        "    Data triples are shuffled after each epoch.\n",
        "    '''\n",
        "    i = self.current_minibatch\n",
        "    di = self.batchsize\n",
        "    mbatch = deepcopy(self.data[i*di:(i+1)*di])\n",
        "    self.current_minibatch += 1\n",
        "    if self.current_minibatch == self.number_minibatches:\n",
        "      np.random.shuffle(self.data)\n",
        "      self.current_minibatch = 0\n",
        "    if self.num_negSamples > 0:\n",
        "      subj, pred, obj, labels = self.apply_neg_examples(list(mbatch[:,0]), list(mbatch[:,1]), list(mbatch[:,2]))\n",
        "      return subj, pred, obj, labels\n",
        "    else:\n",
        "      return mbatch[:,0], mbatch[:,1], mbatch[:,2]\n",
        "\n",
        "  def apply_neg_examples(self, subj, pred, obj):\n",
        "    '''\n",
        "    Generate neg. samples for a mini-batch.\n",
        "    Both subject and object neg. samples are generated.\n",
        "    '''\n",
        "    vsize = len(subj)\n",
        "    labels = np.array([1 for i in range(vsize)] + [-1 for i in range(self.num_negSamples*2*vsize)])\n",
        "    neg_subj = list(np.random.randint(self.num_nodes, size = self.num_negSamples*vsize))\n",
        "    neg_obj = list(np.random.randint(self.num_nodes, size = self.num_negSamples*vsize))\n",
        "    return np.concatenate([subj, neg_subj, subj*self.num_negSamples]), np.concatenate([pred*(2*self.num_negSamples+1)]), np.concatenate([obj, obj*self.num_negSamples, neg_obj]), labels\n"
      ],
      "metadata": {
        "id": "O9s5T6GUa8lb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from argparse import Namespace\n",
        "params_dict = {\n",
        "  'dim': 40,\n",
        "  'input_size': 40,\n",
        "  'tau': 0.5,\n",
        "  'batchsize': 64,\n",
        "  'delta': 0.001,\n",
        "  'lr': .1,\n",
        "  'L2': 0.,\n",
        "  'steps': 801,\n",
        "  'neg_samples': 10,\n",
        "  'maxspan': 2,\n",
        "}\n",
        "params = Namespace(**params_dict)\n",
        "\n",
        "eval_points = list(range(0, 1001, 200))\n",
        "seed = np.random.randint(1e8)\n",
        "\n",
        "batcher = batch_provider(train_data, params.batchsize, params.neg_samples, seed)\n",
        " "
      ],
      "metadata": {
        "id": "lxofEHh70IFK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# first_batch = batcher.next_batch\n",
        "print(batcher.next_batch()[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hPb3ZHCG1SlV",
        "outputId": "124767e8-fe31-4bf5-a973-45fe8db683b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[127  53 231 ... 203 167  66]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How to measure distance"
      ],
      "metadata": {
        "id": "_3D4yARRqI23"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ASYmmetric_score(s_emb, o_emb, p_emb):\n",
        "  ''' TransE score '''\n",
        "  return -F.pairwise_distance(s_emb - o_emb, p_emb, p=1)\n",
        "\n",
        "def SYMmetric_score(s_emb, o_emb, p_emb):\n",
        "  ''' Symmetric version of TransE score '''\n",
        "  return -F.pairwise_distance((s_emb - o_emb).abs(), p_emb, p=1)\n"
      ],
      "metadata": {
        "id": "l1uTGIYFpoLF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "APDDMK98qeQ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(steps, model, optimizer, batcher, data):\n",
        "  if data is not None:\n",
        "    train_data, valid_data, neg_data = data[0], data[1], data[2]\n",
        "  loss_fun = torch.nn.SoftMarginLoss()\n",
        "\n",
        "  start_time = time.time()\n",
        "  for k in range(steps):\n",
        "    if k % 100 == 0:\n",
        "      estimate_time = (time.time()-start_time)/(k+1)*(steps-k)/60.0\n",
        "      if data is not None:\n",
        "        train_score = float(torch.mean(model.score(*train_data)).detach().numpy())\n",
        "        valid_score = float(torch.mean(model.score(*valid_data)).detach().numpy())\n",
        "        neg_score = float(torch.mean(model.score(*neg_data)).detach().numpy())\n",
        "        print('SpikE {}: ETA {}min; train: {}, valid: {}, neg: {}'.format(k, np.round(estimate_time,2), train_score, valid_score, neg_score))\n",
        "      else:\n",
        "        print('SpikE {}: ETA {}min'.format(k, np.round(estimate_time,2)))\n",
        "    optimizer.zero_grad()\n",
        "    databatch = batcher.next_betch()\n",
        "    prediction = model.score(*databatch)\n",
        "    weight_reg = model.entities.weight_loss()\n",
        "    loss = loss_fun(prediction, torch.tensor(databatch[-1]))\n",
        "    loss = loss + delta*weight_reg\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    model.update_embeddings()"
      ],
      "metadata": {
        "id": "SfrM_gEfqVSp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "SVc592X3OCr3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}